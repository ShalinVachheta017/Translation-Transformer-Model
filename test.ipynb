{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s an explanation of **layer normalization** based on the provided image and its context:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Core Idea of Layer Normalization**\n",
    "Layer normalization (LayerNorm) standardizes features **per data instance** (e.g., a token in NLP or a pixel in vision) rather than across a batch. This ensures stability in training, especially for variable-length inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Key Differences: LayerNorm vs. BatchNorm**\n",
    "The image highlights distinct normalization dimensions:\n",
    "- **Batch Normalization (BatchNorm)**:\n",
    "  - Normalizes across **batch and spatial dimensions** (e.g., `H, W, Z` for height, width, depth).\n",
    "  - Computes mean (μ) and variance (σ²) for **each feature channel** over the entire batch.\n",
    "  - Example dimensions: `(N, C, H, W)` → stats computed over `N, H, W` for each channel `C`.\n",
    "- **Layer Normalization (LayerNorm)**:\n",
    "  - Normalizes across **feature dimensions** (e.g., `H, W, Z, X` for all features of a single instance).\n",
    "  - Computes μ and σ² **per instance**, independent of the batch.\n",
    "  - Example dimensions: `(N, H, W, C)` → stats computed over `H, W, C` for each batch item `N`.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Example from the Image**\n",
    "The \"Batch of 3 items\" contains numerical feature values for three instances. Here’s how LayerNorm works for **Item 1**:\n",
    "- **Features**: `[80.40, 2310.625, ..., 840.361, 6.001]`\n",
    "- **Step 1**: Compute μ and σ² across all features of Item 1:\n",
    "  \\[\n",
    "  \\mu_1 = \\frac{1}{d} \\sum_{i=1}^d x_i, \\quad \\sigma_1^2 = \\frac{1}{d} \\sum_{i=1}^d (x_i - \\mu_1)^2\n",
    "  \\]\n",
    "  where \\(d\\) is the number of features.\n",
    "- **Step 2**: Normalize each feature:\n",
    "  \\[\n",
    "  \\hat{x}_j = \\frac{x_j - \\mu_1}{\\sqrt{\\sigma_1^2 + \\epsilon}}\n",
    "  \\]\n",
    "- **Step 3**: Scale and shift with learned parameters:\n",
    "  \\[\n",
    "  \\text{Output}_j = \\gamma \\cdot \\hat{x}_j + \\beta\n",
    "  \\]\n",
    "  (γ and β allow the network to adjust the normalized values.)\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Why LayerNorm in Transformers?**\n",
    "- **Batch Independence**: Unlike BatchNorm, LayerNorm doesn’t rely on batch statistics, making it ideal for variable-length sequences (common in NLP).\n",
    "- **Stability**: Prevents gradient issues in deep networks by normalizing per-instance features.\n",
    "- **Learnable Flexibility**: Parameters γ and β retain model expressiveness.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Visual Summary**\n",
    "| **Normalization Type** | Dimensions Normalized          | Use Case                  |\n",
    "|-------------------------|---------------------------------|--------------------------|\n",
    "| BatchNorm               | Batch (`N`), spatial (`H, W, Z`)| Fixed-size inputs (e.g., images) |\n",
    "| LayerNorm               | Features (`H, W, Z, X`)        | Variable-length inputs (e.g., text) |\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Connection to Transformers**\n",
    "In transformers, LayerNorm is applied **after residual connections** (e.g., post self-attention or feed-forward layers). This stabilizes activations and enables deeper architectures by maintaining consistent feature scales.\n",
    "\n",
    "---\n",
    "\n",
    "**Final Takeaway**: LayerNorm ensures stable training by normalizing features per instance, making it indispensable for transformers and variable-length data. The image contrasts it with BatchNorm, emphasizing its independence from batch statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.version' from 'd:\\\\anaconda\\\\envs\\\\transformer39\\\\lib\\\\site-packages\\\\torch\\\\version.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce GTX 1650\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3, 1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3, 1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
