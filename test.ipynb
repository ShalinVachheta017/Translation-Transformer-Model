{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s an explanation of **layer normalization** based on the provided image and its context:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Core Idea of Layer Normalization**\n",
    "Layer normalization (LayerNorm) standardizes features **per data instance** (e.g., a token in NLP or a pixel in vision) rather than across a batch. This ensures stability in training, especially for variable-length inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Key Differences: LayerNorm vs. BatchNorm**\n",
    "The image highlights distinct normalization dimensions:\n",
    "- **Batch Normalization (BatchNorm)**:\n",
    "  - Normalizes across **batch and spatial dimensions** (e.g., `H, W, Z` for height, width, depth).\n",
    "  - Computes mean (μ) and variance (σ²) for **each feature channel** over the entire batch.\n",
    "  - Example dimensions: `(N, C, H, W)` → stats computed over `N, H, W` for each channel `C`.\n",
    "- **Layer Normalization (LayerNorm)**:\n",
    "  - Normalizes across **feature dimensions** (e.g., `H, W, Z, X` for all features of a single instance).\n",
    "  - Computes μ and σ² **per instance**, independent of the batch.\n",
    "  - Example dimensions: `(N, H, W, C)` → stats computed over `H, W, C` for each batch item `N`.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Example from the Image**\n",
    "The \"Batch of 3 items\" contains numerical feature values for three instances. Here’s how LayerNorm works for **Item 1**:\n",
    "- **Features**: `[80.40, 2310.625, ..., 840.361, 6.001]`\n",
    "- **Step 1**: Compute μ and σ² across all features of Item 1:\n",
    "  \\[\n",
    "  \\mu_1 = \\frac{1}{d} \\sum_{i=1}^d x_i, \\quad \\sigma_1^2 = \\frac{1}{d} \\sum_{i=1}^d (x_i - \\mu_1)^2\n",
    "  \\]\n",
    "  where \\(d\\) is the number of features.\n",
    "- **Step 2**: Normalize each feature:\n",
    "  \\[\n",
    "  \\hat{x}_j = \\frac{x_j - \\mu_1}{\\sqrt{\\sigma_1^2 + \\epsilon}}\n",
    "  \\]\n",
    "- **Step 3**: Scale and shift with learned parameters:\n",
    "  \\[\n",
    "  \\text{Output}_j = \\gamma \\cdot \\hat{x}_j + \\beta\n",
    "  \\]\n",
    "  (γ and β allow the network to adjust the normalized values.)\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Why LayerNorm in Transformers?**\n",
    "- **Batch Independence**: Unlike BatchNorm, LayerNorm doesn’t rely on batch statistics, making it ideal for variable-length sequences (common in NLP).\n",
    "- **Stability**: Prevents gradient issues in deep networks by normalizing per-instance features.\n",
    "- **Learnable Flexibility**: Parameters γ and β retain model expressiveness.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Visual Summary**\n",
    "| **Normalization Type** | Dimensions Normalized          | Use Case                  |\n",
    "|-------------------------|---------------------------------|--------------------------|\n",
    "| BatchNorm               | Batch (`N`), spatial (`H, W, Z`)| Fixed-size inputs (e.g., images) |\n",
    "| LayerNorm               | Features (`H, W, Z, X`)        | Variable-length inputs (e.g., text) |\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Connection to Transformers**\n",
    "In transformers, LayerNorm is applied **after residual connections** (e.g., post self-attention or feed-forward layers). This stabilizes activations and enables deeper architectures by maintaining consistent feature scales.\n",
    "\n",
    "---\n",
    "\n",
    "**Final Takeaway**: LayerNorm ensures stable training by normalizing features per instance, making it indispensable for transformers and variable-length data. The image contrasts it with BatchNorm, emphasizing its independence from batch statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
